{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "import requests\n",
    "\n",
    "\n",
    "from selenium import webdriver as wd \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import platform\n",
    "from IPython.display import Image \n",
    "import pandas as pd\n",
    "import time\n",
    "import re \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/project/bigpycraft/a3rcs/'\n",
    "WD_CHROME_PATH_WIN   = 'driver/chromedriver.exe'\n",
    "WD_CHROME_PATH_MAC   = 'driver/macos/chromedriver'\n",
    "WD_CHROME_PATH_LINUX = 'driver/linux/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    '''\n",
    "        웹드라이브를 호출하는 메소드\n",
    "        parmam : None\n",
    "        Result : driver  , ChromeWebDriver \n",
    "    ''' \n",
    "    \n",
    "    print('WD_PATH:', PROJECT_ROOT + WD_CHROME_PATH_WIN)\n",
    "    if platform.system() == 'Windows':    # MacOS\n",
    "        driver = wd.Chrome(executable_path=PROJECT_ROOT + WD_CHROME_PATH_WIN)\n",
    "    elif platform.system() == 'Darwin': # Windows\n",
    "        driver = wd.Chrome(executable_path=PROJECT_ROOT + WD_CHROME_PATH_MAC)\n",
    "    elif platform.system() == 'Linux': # Windows\n",
    "        driver = wd.Chrome(executable_path=PROJECT_ROOT + WD_CHROME_PATH_LINUX)\n",
    "    else:\n",
    "         print(\"It's unknown system. Hangul fonts are not supported!\")\n",
    "\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    '''\n",
    "        BeutiulSoup 객체를 가져오는 메소드\n",
    "        parmam : url     , 가져올 URL주소\n",
    "        Result : soup    , BeutiulSoup \n",
    "    ''' \n",
    "    http = urlopen(url)\n",
    "    soup = BeautifulSoup(http, 'html.parser')\n",
    "    \n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def soup_url(url):\n",
    "    http = urlopen(url)\n",
    "    soup = BeautifulSoup(http, 'html.parser')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def switch_url(url):\n",
    "    '''\n",
    "    http://blog.naver.com/ 주소에 한정된 iframe switching 모듈입니다.\n",
    "    iframe 전환 및 blog 메뉴로 곧바로 접속됩니다.\n",
    "    '''\n",
    "\n",
    "    re_url = url.replace('com/', 'com/PostList.nhn?blogId=')+'&directAccess=true&userTopListOpen=false&userTopListCount=30&userTopListCurrentPage=1'\n",
    "    \n",
    "    return re_url\n",
    "\n",
    "def blog_basic_info(re_url):\n",
    "    '''\n",
    "    스위칭된 url로 기본 정보를 가져온다.\n",
    "    \n",
    "    Args:\n",
    "        re_url : 스윗칭된 url주소, str\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pattern = re.compile('blogId=\\w+')\n",
    "    b_id    = pattern.search(re_url)\n",
    "    bloger_id = b_id.group().split('=')[1]\n",
    "    \n",
    "    blog_type = 'NV'  # 네이버 블로그 모듈이기에 상수다\n",
    "    \n",
    "    blog_basic =[bloger_id, blog_type]\n",
    "    return blog_basic\n",
    "\n",
    "\n",
    "def url_collect(driver):\n",
    "    '''\n",
    "    게시글 목록이 열려진 페이지에서 url을 수집하는 모듈입니다.\n",
    "    list로 반환\n",
    "    '''\n",
    "    url_list = []\n",
    "    \n",
    "    url_collect = driver.find_element_by_xpath('//*[@id=\"listTopForm\"]/table')\n",
    "    url_a = url_collect.find_elements_by_tag_name('a')\n",
    "    \n",
    "    for u in url_a:\n",
    "        url_list.append(u.get_attribute('href'))\n",
    "        \n",
    "    return url_list\n",
    "\n",
    "def last_page(driver):\n",
    "    time.sleep(2)   # 드라이버를 열고 바로 이 모듈을 호출할 경우 데이터를 못 불러와 에러남\n",
    "    all_p = driver.find_element_by_xpath('//*[@id=\"category-name\"]/div/table[2]/tbody/tr/td[2]/div/h4').text\n",
    "    posts = all_p.split(' ')[1][:-2]\n",
    "    all_post = int(posts.replace(',',''))\n",
    "   \n",
    "    # 30줄 보기 버전\n",
    "    \n",
    "    if all_post % 30 == 0:\n",
    "        last_page = all_post // 30\n",
    "    else:\n",
    "        last_page = (all_post // 30) + 1\n",
    "        \n",
    "    print('총 ', all_post,'개의 글이 있으며 마지막 페이지는 ', last_page,'입니다.')  \n",
    "    \n",
    "    return last_page\n",
    "\n",
    "# def url_first_10page(driver):\n",
    "#     '''\n",
    "#     처음 10페이지까지의 url을 전부 수집한다. 그리고 11페이지로 넘긴다.\n",
    "#     url_collect 모듈이 사용된다.\n",
    "#     Args \n",
    "#         driver : 크롬 웹 드라이버\n",
    "        \n",
    "#     Return\n",
    "#         url_list : url정보가 담긴 리스트\n",
    "    \n",
    "#     '''\n",
    "#     url_list = url_collect(driver)               \n",
    "        \n",
    "#     page_list = driver.find_element_by_xpath('//*[@id=\"toplistWrapper\"]/div[2]/div').text\n",
    "#     page_list = page_list.split('\\n')\n",
    "    \n",
    "#     if page_list[-1] == '다음':\n",
    "#         for i in range(1, 10):\n",
    "#             path = '//*[@id=\"toplistWrapper\"]/div[2]/div/a[{}]'.format(i)  \n",
    "#             driver.find_element_by_xpath(path).click()\n",
    "            \n",
    "#             time.sleep(2)\n",
    "            \n",
    "#             url_list.extend(url_collect(driver))               \n",
    "            \n",
    "#     else :\n",
    "#         pages = [i + 1 for i in range(len(page_li)-1)]\n",
    "#         path = '//*[@id=\"toplistWrapper\"]/div[2]/div/a[{}]'.format(page_list[i])  \n",
    "#         driver.find_element_by_xpath(path).click()\n",
    "            \n",
    "#         time.sleep(2)\n",
    "            \n",
    "#         url_list.extend(url_collect(driver))\n",
    "    \n",
    "#     driver.find_element_by_xpath('//*[@id=\"toplistWrapper\"]/div[2]/div/a[10]').click()\n",
    "                                 \n",
    "#     return url_list\n",
    "\n",
    "def make_DF_nv():\n",
    "    df = pd.DataFrame()\n",
    "    df['blog_type'] = ''\n",
    "    df['bloger_id'] = ''\n",
    "    df['post_num'] = ''\n",
    "    df['key'] = ''\n",
    "    df['url'] = ''\n",
    "    df['title'] = ''\n",
    "    df['content'] = ''\n",
    "    df['category'] = ''\n",
    "    df['bloger_nick'] = ''\n",
    "    df['date'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def all_page_urls(re_url, last_page):   \n",
    "    '''\n",
    "    글목록 url 주소를 받아 마지막페이지까지 돌면서\n",
    "    url_collect 모듈을 사용,\n",
    "    전체 글 url을 수집한다.\n",
    "    \n",
    "    Args:\n",
    "        re_url     : 글목록 url주소, str\n",
    "        last_page  : 글목록 마지막 페이지, int\n",
    "\n",
    "    Return:\n",
    "        url_list   : 전체 글 url 정보 list\n",
    "       \n",
    "    ''' \n",
    "    url_list = list()\n",
    "\n",
    "    for i in range(last_page):\n",
    "        re_url = re_url.replace('Page=1', 'Page={0}')\n",
    "        new_url = re_url.format(i+1)\n",
    "        driver.get(new_url)\n",
    "        time.sleep(1)\n",
    "        url_lis = url_collect(driver)\n",
    "        url_list.extend(url_lis)\n",
    "     \n",
    "    print('총 ',len(url_list),'개의 url을 수집하였습니다.')\n",
    "    \n",
    "    return url_list\n",
    "   \n",
    "\n",
    "# 일단 parser\n",
    "\n",
    "# def parser(soup):   # 구형파서\n",
    "#     '''\n",
    "#     soup module에서 가져온 soup 정보를 통해\n",
    "#     글쓴이, 업로드 날짜, 제목, 내용을 가져옵니다.\n",
    "#     return의 형태는 dictionary 입니다.\n",
    "#     '''\n",
    "    \n",
    "#     import re\n",
    "    \n",
    "#     info = {}   # 정보를 담을 딕셔너리\n",
    "    \n",
    "#     # 페이지 내 본문 parser\n",
    "#     all_content = soup.find('div',class_='se-viewer se-theme-default')\n",
    "    \n",
    "#     nickname = all_content.find('span', class_='nick').text                                                     # 글쓴이\n",
    "#     upload_date = all_content.find('span', class_='se_publishDate pcol2').text                                  # 날짜\n",
    "#     title = re.sub('\\n','',all_content.find('div', class_='se-module se-module-text se-title-text').text)       # 제목\n",
    "#     content = re.sub('\\n|\\u200b','',all_content.find('div', class_='se-main-container').text)                   # 본문\n",
    "    \n",
    "#     info['nickname'] = nickname\n",
    "#     info['upload_date'] = upload_date\n",
    "#     info['title'] = title\n",
    "#     info['content'] = content\n",
    "\n",
    "#     return info\n",
    "\n",
    "\n",
    "def parser(url_list):    # 신형 파서\n",
    "    '''\n",
    "    soup module에서 가져온 url_list(list)를 통해\n",
    "    글쓴이, 업로드 날짜, 제목, 내용을 가져옵니다.\n",
    "    return의 형태는 dictionary 입니다.\n",
    "    '''\n",
    "    \n",
    "    info = {}   # 정보를 담을 딕셔너리\n",
    "    nick = list()\n",
    "    date = list()\n",
    "    cate = list()\n",
    "    tit = list()\n",
    "    con = list()\n",
    "    \n",
    "    # 페이지 내 본문 parser\n",
    "    for u in url_list:\n",
    "        s = requests.Session()\n",
    "        s.mount(u, HTTPAdapter(max_retries=3))\n",
    "        r = s.get(u)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        news_list = soup.find('div', {'class' : \"postSubjectContent\"})\n",
    "        time.sleep(3)\n",
    "\n",
    "        all_content = soup.find('div',class_='se-viewer se-theme-default')\n",
    "\n",
    "        if bool(all_content) == True:\n",
    "            nickname = all_content.find('span', class_='nick').text                                                     # 글쓴이\n",
    "            upload_date = all_content.find('span', class_='se_publishDate pcol2').text                                  # 날짜\n",
    "            category = re.sub('\\n','',all_content.find('div', class_='blog2_series').text)\n",
    "            title = re.sub('\\n','',all_content.find('div', class_='se-module se-module-text se-title-text').text)       # 제목\n",
    "            content = re.sub('\\n|\\u200b','',all_content.find('div', class_='se-main-container').text)                   # 본문\n",
    "            url = u \n",
    "        else:\n",
    "            else_all_content = soup.find('div', id='postListBody')\n",
    "            nickname = nick[0]\n",
    "            upload_date = else_all_content.find('p', class_='date fil5 pcol2 _postAddDate').text \n",
    "            category = re.sub('\\t|\\n|\\xa0','',else_all_content.find('span', class_='cate pcol2').text)\n",
    "            title = re.sub('\\n','',else_all_content.find('span', class_='pcol1 itemSubjectBoldfont').text)\n",
    "            content = re.sub('\\n|\\u200b','',else_all_content.find('div', id='postViewArea').text)   \n",
    "            \n",
    "            \n",
    "        nick.append(nickname)\n",
    "        date.append(upload_date)\n",
    "        cate.append(category)\n",
    "        tit.append(title)\n",
    "        con.append(content)\n",
    "        \n",
    "    info['nickname'] = nick\n",
    "    info['date'] = date\n",
    "    info['category'] = cate\n",
    "    info['title'] = tit\n",
    "    info['content'] = con\n",
    "    \n",
    "    return info\n",
    "\n",
    "def basic_DF(naver_blog_DF, blog_info, url_list):\n",
    "    naver_blog_DF['blog_type'] = blog_info[1]\n",
    "    naver_blog_DF['bloger_id'] = blog_info[0]\n",
    "    naver_blog_DF['url'] = url_list\n",
    "    \n",
    "    pattern = re.compile('logNo=\\w+')\n",
    "    for u in range(0, len(url_list)):\n",
    "        b_num    = pattern.search(url_list[u])\n",
    "        post_num = b_num.group().split('=')[1]\n",
    "        naver_blog_DF['post_num'][u] = post_num\n",
    "        \n",
    "    for i in range(0, len(naver_blog_DF)):\n",
    "        naver_blog_DF['key'][i] = blog_info[1]+\"_\"+blog_info[0]+\"_\"+naver_blog_DF['post_num'][i]\n",
    "    \n",
    "    return naver_blog_DF\n",
    "\n",
    "\n",
    "def complete_NB_DF(naver_blog_DF,info ):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "#     naver_blog_DF['blog_type'] = bloger_id\n",
    "#     naver_blog_DF['bloger_id'] = blog_type\n",
    "#     naver_blog_DF['post_num'] = ''\n",
    "#     naver_blog_DF['key'] = blog_type + '_' + bloger_id + '_' # + info['post_num']\n",
    "#     naver_blog_DF['url'] = ''\n",
    "    naver_blog_DF.loc[:,'title'] = pd.Series(info['title'])\n",
    "    naver_blog_DF.loc[:,'content'] = pd.Series(info[ 'content'])\n",
    "    naver_blog_DF.loc[:,'category'] = pd.Series(info['category'])\n",
    "    naver_blog_DF.loc[:,'bloger_nick'] = pd.Series(info['nickname'])\n",
    "    naver_blog_DF.loc[:,'date'] = pd.Series(info['date'])\n",
    "    \n",
    "    return naver_blog_DF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

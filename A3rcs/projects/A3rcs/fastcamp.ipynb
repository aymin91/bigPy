{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from konlpy.tag import Mecab\n",
    "import string\n",
    "import warnings\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(input_file_name):\n",
    "    '''리스트로 변환'''\n",
    "    corpus = list()\n",
    "    \n",
    "    with open(input_file_name, 'rb') as f:\n",
    "        temp_corpus = pickle.load(f)\n",
    "        \n",
    "    for page in temp_corpus:\n",
    "        corpus += page\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def text_cleaning(docs):\n",
    "    '''한국어를 제외한 글자를 제거하는 함수'''\n",
    "    cleaned_docs = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        temp_doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "        cleaned_docs.append(temp_doc)\n",
    "        \n",
    "    return cleaned_docs\n",
    "\n",
    "def define_stopwords(path):\n",
    "    SW = set()\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "            \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    token_corpus = list()\n",
    "    \n",
    "    if tokenizer == 'noun':\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc='Preprocessing'):\n",
    "            token_text = mecab.nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word)>1]\n",
    "            \n",
    "            token_corpust.append(token_text)\n",
    "            \n",
    "    elif tokenizer == 'morph':\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc='Preprocessing'):\n",
    "            token_text = mecab.morphs(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word)>1]\n",
    "            \n",
    "            token_corpust.append(token_text)\n",
    "        \n",
    "    elif tokenizer == 'word':\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc='Preprocessing'):\n",
    "            token_text = corpus[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word)>1]\n",
    "            \n",
    "            token_corpust.append(token_text)\n",
    "    \n",
    "    \n",
    "    return token_corpus\n",
    "\n",
    "input_file_name = '/naver_news_content.pk'\n",
    "documents = read_documents(input_file_name)\n",
    "SW = define_stopwords('/stopwords-ko.txt')\n",
    "cleaned_text = text_cleaning(documents)\n",
    "tokenized_text = text_tokenizing(cleaned_text, tokenizer = 'noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 모델링에 사용할 함수들 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서-단어 행렬만들기\n",
    "# 어휘(vocabulary) 학습\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "# 문서-단어 행렬(document-term matrix) 생성\n",
    "corpus = [dictionary.doc2bow(text) for text n tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 확인\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 확인\n",
    "corpus[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf 문서-단어 행렬 생성\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "print(corpus_tfidf[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda model 만들기\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda 결과 확인\n",
    "model.show_topid(topic_num = 0, n_top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 모델링을 추가하여 코드 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토픽 개수, 키워드 개수를 정해주는 변수 추가\n",
    "NUM_TOPICS = 3\n",
    "NUM_TOPIC_WORDS = 30\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "    \n",
    "    return corpus, dictionary\n",
    "\n",
    "def print_topic_word(model):\n",
    "    \n",
    "    for topic_id in range(model.num_topics):\n",
    "        topic_word_probs = model.show_topic(topic_id, NUM_TOPIC_WORDS)\n",
    "        print('Topic ID: {}'.format(topic_id))\n",
    "        \n",
    "        for topic_word, prob in topic_word_probs:\n",
    "            print('\\t{}\\t{}'.format(topic_word, prob))\n",
    "    \n",
    "        print('\\n')\n",
    "        \n",
    "    return None\n",
    "\n",
    "# dictionary-term matrix 만들기\n",
    "corpus, dictionary = build_doc_term_mat(tokenized_text)\n",
    "# LDA 실행\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary, alpha = 'auto', eta='auto')\n",
    "# 결과 출력\n",
    "print_topic_word(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyLDAvis를 통한 토픽 모델링 결과 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (1.2.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (0.25.3)\n",
      "Collecting joblib>=0.8.4\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: numexpr in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (2.6.9)\n",
      "Requirement already satisfied: pytest in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: future in c:\\anaconda\\lib\\site-packages (from pyLDAvis) (0.17.1)\n",
      "Collecting funcy\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\anaconda\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\anaconda\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (1.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (40.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (0.9.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (6.0.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from pytest->pyLDAvis) (0.4.1)\n",
      "Building wheels for collected packages: pyLDAvis, funcy\n",
      "  Building wheel for pyLDAvis (setup.py): started\n",
      "  Building wheel for pyLDAvis (setup.py): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97717 sha256=019661e0c0478a57fc80ec7f1217e11e306988aeccfecc22370d08d6fd8fb920\n",
      "  Stored in directory: C:\\Users\\민아영\\AppData\\Local\\pip\\Cache\\wheels\\98\\71\\24\\513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
      "  Building wheel for funcy (setup.py): started\n",
      "  Building wheel for funcy (setup.py): finished with status 'done'\n",
      "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32045 sha256=2b76713c3d59f2ac72c7f9b38c037dade8c6a1c1f85b928b89c76415f0bf6594\n",
      "  Stored in directory: C:\\Users\\민아영\\AppData\\Local\\pip\\Cache\\wheels\\20\\5a\\d8\\1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
      "Successfully built pyLDAvis funcy\n",
      "Installing collected packages: joblib, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.14 joblib-0.14.1 pyLDAvis-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis 불러오기\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# pyLDAvis를 jupyter notebook에서 실행할 수 있게 활성화\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# pyLDAvis 실행\n",
    "data = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NSMC 불러와서 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(filename):\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        documents = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        \n",
    "        \n",
    "    \n",
    "    return documents\n",
    "\n",
    "train_docs = read_documents('ratings_train.txt')\n",
    "test_docs  = read_documents('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_docs))\n",
    "print(len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(docs):\n",
    "    '''한국어를 제외한 글자를 제거하는 함수'''\n",
    "    cleaned_docs = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        temp_doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "        cleaned_docs.append(temp_doc)\n",
    "        \n",
    "    return cleaned_docs\n",
    "\n",
    "def define_stopwords(path):\n",
    "    SW = set()\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "            \n",
    "    return SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
